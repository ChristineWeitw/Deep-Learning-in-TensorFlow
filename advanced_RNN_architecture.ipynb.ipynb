{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN - Sentiment Predictions\n",
    "##### we'll be looking at sequential data in the form of text data of movie reviews on the Internet Movie Database (IMDB) website. We'll be training our models on this data and using it to predict the sentiment (either positive or negative) of the reviews.\n",
    "##### The data contains 50000 rows of movie reviews (evenly split between train and test) where each word in the review has been converted to numeric values (integers) as part of the pre-processing. Both the train and test datasets are balanced;\n",
    "-------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (Long Short-Term Memory)\n",
    "   * Gated units (nodes) that control what information should be retained and what should be forgotten.\n",
    "   * LSTM has three gates: input, output, and forget.\n",
    "        1. Fotget gate : what information from the network's memory so far should be forgotten.\n",
    "        2. Input gate : what from the new information that is coming in should be stored in memory. \n",
    "        3. Output gate : what information should be passed along to the next step, takes into account both the memory and the new information.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 3)                 432       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 12        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 448\n",
      "Trainable params: 448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.LSTM(3, input_shape=(200, 32)))\n",
    "model.add(layers.Dense(3, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------\n",
    "\n",
    "## GRU (Gated Recurrent Units)\n",
    "   * Gated units (nodes) that control what information should be retained and what should be forgotten.\n",
    "   * GRU has only 2 gates: reset and update.\n",
    "        1. Reset gate : old information dropped.\n",
    "        2. Updaqt4e gate : new information passed along and/or stored in memory.\n",
    "   * The gates in the GRU are for controlling what old information gets dropped from memory as new information comes in (reset gate) and what new information gets passed along and/or stored in memory (update gate). In general, because GRUs have a simpler architecture (fewer gates) than LSTM, they use less memory and tend to run faster. That being said, LSTM may be more accurate when used on longer sequences. [--dataqu4est.io]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_1 (GRU)                 (None, 3)                 333       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 12        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 349\n",
      "Trainable params: 349\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define model \n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.GRU(3, input_shape=(200,32)))\n",
    "model.add(layers.Dense(3, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preporcessing \n",
    "    --Embedding Layer:\n",
    "   * represent the words in our data as fixed-length n-dimensional vectors that can extract meaning and context from our text.\n",
    "   * learns to map integers (which represent words) to dense vectors (called embeddings) such that the semantic relationships between words are captured in the geometric relationships between the vectors.\n",
    "   * The embedding layer is able to place related words closer together in a multi-dimensional space which enables the model to extract meaning and context from a sequence of words.\n",
    "   * Most commonly used with text data. Usually is the first layer in the model.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.000e+00, 0.000e+00, 0.000e+00, ..., 8.740e+02, 1.450e+02,\n",
       "        1.000e+01],\n",
       "       [0.000e+00, 0.000e+00, 0.000e+00, ..., 3.200e+01, 3.100e+01,\n",
       "        4.700e+01],\n",
       "       [0.000e+00, 0.000e+00, 0.000e+00, ..., 9.000e+00, 6.176e+03,\n",
       "        4.700e+01],\n",
       "       ...,\n",
       "       [7.629e+03, 3.700e+01, 1.100e+01, ..., 1.563e+03, 1.467e+03,\n",
       "        5.600e+01],\n",
       "       [0.000e+00, 0.000e+00, 0.000e+00, ..., 1.321e+03, 2.300e+01,\n",
       "        4.700e+01],\n",
       "       [3.875e+03, 5.000e+00, 3.100e+01, ..., 1.000e+01, 1.295e+03,\n",
       "        4.300e+01]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data\n",
    "X_train_prep = np.loadtxt(\"imdb_X_train_prep_200.csv\", delimiter=',')\n",
    "X_test_prep = np.loadtxt(\"imdb_X_test_prep_200.csv\", delimiter=',')\n",
    "y_train_prep = np.loadtxt(\"imdb_y_train_prep_200.csv\", delimiter=',')\n",
    "y_test_prep = np.loadtxt(\"imdb_y_test_prep_200.csv\", delimiter=',')\n",
    "\n",
    "X_train_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 200)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_prep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 200, 32)           800000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 3)                 432       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 12        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 800,448\n",
      "Trainable params: 800,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate initial model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Embedding(input_dim=25000, output_dim=32, input_length=200))\n",
    "model.add(layers.LSTM(3))\n",
    "model.add(layers.Dense(3, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Output Shape of the embedding layer in the model summary is the input for the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 40s 49ms/step - loss: 0.6931\n",
      "782/782 [==============================] - 8s 10ms/step\n",
      "Accuracy Score on the test set: 0.5176\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_prep, y_train_prep)\n",
    "y_pred = model.predict(X_test_prep)\n",
    "\n",
    "from sklearn.metrics import accuracy_score \n",
    "print(f\"Accuracy Score on the test set: {accuracy_score(y_test_prep, np.round(y_pred))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49974284],\n",
       "       [0.50161827],\n",
       "       [0.503109  ],\n",
       "       ...,\n",
       "       [0.5031221 ],\n",
       "       [0.50764626],\n",
       "       [0.4996824 ]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing an LSTM Model -- by adjusting \"embedding dimension (output_dim) used in the Embedding layer\"\n",
    "There are a number of different ways to experiment to try to improve the performance of our model. Some of these include:\n",
    "\n",
    "* Number of hidden layers\n",
    "* Number of nodes per hidden layer\n",
    "* Activation function used (or not) at each layer\n",
    "* Optimizer\n",
    "* Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_dict = {'sgd': 0.5176,\n",
    "                 'adam': 0.81208,\n",
    "                 'RMSprop': 0.81552,\n",
    "                 'output_dim-1': 0.5,\n",
    "                 'output_dim-2': 0.80616,\n",
    "                 'output_dim-4': 0.81192,\n",
    "                 'output_dim-8': 0.73832,\n",
    "                 'output_dim-16': 0.83192,\n",
    "                 'output_dim-32': 0.84692,\n",
    "                 'output_dim-64': 0.8236,\n",
    "                 'output_dim-128': 0.82832,\n",
    "                 'number_of_hidden-layers-3': 0.8358,\n",
    "                 'number_of_hidden-layers-4': 0.78588,\n",
    "                 'number_of_hidden-layers-5': 0.59844,\n",
    "                 'number_of_nodes_2_hidden_layers-4': 0.8484,\n",
    "                 'number_of_nodes_3_hidden_layers-4': 0.85452,\n",
    "                 'number_of_nodes_3_hidden_layers-5': 0.82284,\n",
    "                 'number_of_nodes_2_hidden_layers-5': 0.84496,\n",
    "                 'number_of_nodes_2_hidden_layers-6': 0.82944,\n",
    "                 'number_of_nodes_3_hidden_layers-6': 0.84072,\n",
    "                 'number_of_nodes_2_hidden_layers-32': 0.85864,\n",
    "                 'number_of_nodes_2_hidden_layers-adam_32': 0.86888}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 200, 32)           800000    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 32)                8320      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 810,497\n",
      "Trainable params: 810,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "782/782 [==============================] - 67s 83ms/step - loss: 0.4349\n",
      "782/782 [==============================] - 11s 14ms/step\n",
      "Accuracy Score on the test set: 0.86768\n"
     ]
    }
   ],
   "source": [
    "# Instantiate initial model\n",
    "optimizer_ = 'adam'\n",
    "output_dim_ = 2**5\n",
    "# hidden_layers = 5\n",
    "nodes_hidden_layers = 64\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Embedding(input_dim=25000, output_dim=output_dim_, input_length=200))\n",
    "model.add(layers.LSTM(32))\n",
    "model.add(layers.Dense(nodes_hidden_layers, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=optimizer_, loss='binary_crossentropy')\n",
    "model.summary()\n",
    "model.fit(X_train_prep, y_train_prep)\n",
    "y_pred = model.predict(X_test_prep)\n",
    "\n",
    "from sklearn.metrics import accuracy_score \n",
    "print(f\"Accuracy Score on the test set: {accuracy_score(y_test_prep, np.round(y_pred))}\")\n",
    "\n",
    "optimize_formula = f'number_of_nodes_2_hidden_layers-adam_{nodes_hidden_layers}'\n",
    "accuracy = accuracy_score(y_test_prep, np.round(y_pred))\n",
    "optimize_dict[optimize_formula] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sgd': 0.5176,\n",
       " 'adam': 0.81208,\n",
       " 'RMSprop': 0.81552,\n",
       " 'output_dim-1': 0.5,\n",
       " 'output_dim-2': 0.80616,\n",
       " 'output_dim-4': 0.81192,\n",
       " 'output_dim-8': 0.73832,\n",
       " 'output_dim-16': 0.83192,\n",
       " 'output_dim-32': 0.84692,\n",
       " 'output_dim-64': 0.8236,\n",
       " 'output_dim-128': 0.82832,\n",
       " 'number_of_hidden-layers-3': 0.8358,\n",
       " 'number_of_hidden-layers-4': 0.78588,\n",
       " 'number_of_hidden-layers-5': 0.59844,\n",
       " 'number_of_nodes_2_hidden_layers-4': 0.8484,\n",
       " 'number_of_nodes_3_hidden_layers-4': 0.85452,\n",
       " 'number_of_nodes_3_hidden_layers-5': 0.82284,\n",
       " 'number_of_nodes_2_hidden_layers-5': 0.84496,\n",
       " 'number_of_nodes_2_hidden_layers-6': 0.82944,\n",
       " 'number_of_nodes_3_hidden_layers-6': 0.84072,\n",
       " 'number_of_nodes_2_hidden_layers-32': 0.85864,\n",
       " 'number_of_nodes_2_hidden_layers-adam_32': 0.86888,\n",
       " 'number_of_nodes_2_hidden_layers-adam_64': 0.86732}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_dict['number_of_nodes_3_hidden_layers-6'] = 0.84072"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 1, 32)             800000    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 32)                8320      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 810,497\n",
      "Trainable params: 810,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "782/782 [==============================] - 10s 10ms/step - loss: 0.6919\n",
      "782/782 [==============================] - 1s 1ms/step\n",
      "Accuracy Score on the test set: 0.49968\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 120, 32)           800000    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 32)                8320      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 810,497\n",
      "Trainable params: 810,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "782/782 [==============================] - 44s 54ms/step - loss: 0.4588\n",
      "782/782 [==============================] - 8s 10ms/step\n",
      "Accuracy Score on the test set: 0.83772\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 200, 32)           800000    \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 32)                8320      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 810,497\n",
      "Trainable params: 810,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "782/782 [==============================] - 67s 84ms/step - loss: 0.4335\n",
      "782/782 [==============================] - 11s 14ms/step\n",
      "Accuracy Score on the test set: 0.86464\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 500, 32)           800000    \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 32)                8320      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 810,497\n",
      "Trainable params: 810,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "782/782 [==============================] - 155s 196ms/step - loss: 0.4080\n",
      "782/782 [==============================] - 30s 39ms/step\n",
      "Accuracy Score on the test set: 0.85344\n"
     ]
    }
   ],
   "source": [
    "sequence_lengths = [1, 120, 200, 500]\n",
    "\n",
    "# create 4 dicts that store 4 datasets X_train, y_train, X_text, y_test\n",
    "X_train_prep_dict = {}\n",
    "X_test_prep_dict = {}\n",
    "y_train_prep_dict = {}\n",
    "y_test_prep_dict = {}\n",
    "results_sequence_lengths = {}\n",
    "\n",
    "for sequence_length in sequence_lengths:  \n",
    "    X_train_prep_dict[sequence_length] = np.loadtxt(f\"imdb_X_train_prep_{sequence_length}.csv\", delimiter=\",\")\n",
    "    X_test_prep_dict[sequence_length] = np.loadtxt(f\"imdb_X_test_prep_{sequence_length}.csv\", delimiter=\",\")\n",
    "    y_train_prep_dict[sequence_length] = np.loadtxt(f\"imdb_y_train_prep_{sequence_length}.csv\", delimiter=\",\")\n",
    "    y_test_prep_dict[sequence_length] = np.loadtxt(f\"imdb_y_test_prep_{sequence_length}.csv\", delimiter=\",\")\n",
    "    \n",
    "\n",
    "    model = tf.keras.Sequential()   # model initialion\n",
    "    model.add(layers.Embedding(input_dim=25000, output_dim=output_dim_, input_length=sequence_length))   # embedding layer\n",
    "    model.add(layers.LSTM(32))   # hidden layer\n",
    "    model.add(layers.Dense(64, activation='relu'))   # hidden layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))   # output layer\n",
    "\n",
    "    model.compile(optimizer=optimizer_, loss='binary_crossentropy')\n",
    "    model.summary()\n",
    "    model.fit(X_train_prep_dict[sequence_length], y_train_prep_dict[sequence_length])\n",
    "    y_pred = model.predict(X_test_prep_dict[sequence_length])\n",
    "\n",
    "    from sklearn.metrics import accuracy_score \n",
    "    print(f\"Accuracy Score on the test set: {accuracy_score(y_test_prep_dict[sequence_length], np.round(y_pred))}\")\n",
    "\n",
    "    accuracy = accuracy_score(y_test_prep, np.round(y_pred))\n",
    "    results_sequence_lengths[f'sequence_lengths_{sequence_length}'] = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison - SimpleRNN & LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 200, 32)           800000    \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 804,257\n",
      "Trainable params: 804,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "782/782 [==============================] - 40s 50ms/step - loss: 0.6144\n",
      "782/782 [==============================] - 8s 10ms/step\n",
      "Accuracy Score on the test set: 0.80812\n"
     ]
    }
   ],
   "source": [
    "# Instantiate initial model\n",
    "optimizer_ = 'adam'\n",
    "output_dim_ = 2**5\n",
    "# hidden_layers = 5\n",
    "nodes_hidden_layers = 64\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Embedding(input_dim=25000, output_dim=output_dim_, input_length=200))\n",
    "model.add(layers.SimpleRNN(32))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=optimizer_, loss='binary_crossentropy')\n",
    "model.summary()\n",
    "model.fit(X_train_prep, y_train_prep)\n",
    "y_pred = model.predict(X_test_prep)\n",
    "\n",
    "from sklearn.metrics import accuracy_score \n",
    "print(f\"Accuracy Score on the test set: {accuracy_score(y_test_prep, np.round(y_pred))}\")\n",
    "\n",
    "optimize_formula = f'number_of_nodes_2_SimpleRNN_hidden_layers-adam_{nodes_hidden_layers}'\n",
    "accuracy = accuracy_score(y_test_prep, np.round(y_pred))\n",
    "optimize_dict[optimize_formula] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sgd': 0.5176,\n",
       " 'adam': 0.81208,\n",
       " 'RMSprop': 0.81552,\n",
       " 'output_dim-1': 0.5,\n",
       " 'output_dim-2': 0.80616,\n",
       " 'output_dim-4': 0.81192,\n",
       " 'output_dim-8': 0.73832,\n",
       " 'output_dim-16': 0.83192,\n",
       " 'output_dim-32': 0.84692,\n",
       " 'output_dim-64': 0.8236,\n",
       " 'output_dim-128': 0.82832,\n",
       " 'number_of_hidden-layers-3': 0.8358,\n",
       " 'number_of_hidden-layers-4': 0.78588,\n",
       " 'number_of_hidden-layers-5': 0.59844,\n",
       " 'number_of_nodes_2_hidden_layers-4': 0.8484,\n",
       " 'number_of_nodes_3_hidden_layers-4': 0.85452,\n",
       " 'number_of_nodes_3_hidden_layers-5': 0.82284,\n",
       " 'number_of_nodes_2_hidden_layers-5': 0.84496,\n",
       " 'number_of_nodes_2_hidden_layers-6': 0.82944,\n",
       " 'number_of_nodes_3_hidden_layers-6': 0.84072,\n",
       " 'number_of_nodes_2_hidden_layers-32': 0.85864,\n",
       " 'number_of_nodes_2_hidden_layers-adam_32': 0.86888,\n",
       " 'number_of_nodes_2_SimpleRNN_hidden_layers-adam_64': 0.80812}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}


